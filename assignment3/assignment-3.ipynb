{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment Module 3: Quantum Neural Networks (QNN)\n",
        "\n",
        "This assignment focuses on quantum neural networks (QNNs) for binary classification using a PennyLane + PyTorch implementation of a fully quantum classifier. To complete the assignment, you will implement missing code (marked **YOUR CODE HERE**) and answer a set of short theoretical questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation\n",
        "\n",
        "* Look at the notebooks and slides on **Quantum Neural Networks (QNNs)** and **PennyLane-PyTorch integration** provided in this module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use of generative AI tools\n",
        "\n",
        "You may use AI-based tools (e.g., ChatGPT, GitHub Copilot, Claude, Gemini, DeepSeek, ...) for brainstorming, refactoring, coding assistance, plotting, or editing.\n",
        "\n",
        "This is allowed with disclosure. LLMs are a great tool, but you have to make sure to grasp the contents of the course!\n",
        "\n",
        "**Make sure to fill in the mandatory AI-disclosure in the notebook before submitting!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparatory code\n",
        "\n",
        "Run this cell to import the modules we need.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "!pip install pennylane\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\r\n",
            "  Downloading pennylane-0.43.1-py3-none-any.whl.metadata (11 kB)\r\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/site-packages (from pennylane) (1.16.1)\r\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from pennylane) (3.3)\r\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\r\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\r\n",
            "Collecting autograd (from pennylane)\r\n",
            "  Downloading autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\r\n",
            "Collecting appdirs (from pennylane)\r\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\r\n",
            "Collecting autoray==0.8.0 (from pennylane)\r\n",
            "  Downloading autoray-0.8.0-py3-none-any.whl.metadata (6.1 kB)\r\n",
            "Collecting cachetools (from pennylane)\r\n",
            "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\r\n",
            "Collecting pennylane-lightning>=0.43 (from pennylane)\r\n",
            "  Downloading pennylane_lightning-0.43.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from pennylane) (2.32.5)\r\n",
            "Collecting tomlkit (from pennylane)\r\n",
            "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\r\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/site-packages (from pennylane) (4.12.2)\r\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from pennylane) (25.0)\r\n",
            "Collecting diastatic-malt (from pennylane)\r\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from pennylane) (2.1.2)\r\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.43->pennylane)\r\n",
            "  Downloading scipy_openblas32-0.3.30.0.8-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\r\n",
            "Collecting astunparse (from diastatic-malt->pennylane)\r\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\r\n",
            "Collecting gast (from diastatic-malt->pennylane)\r\n",
            "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\r\n",
            "Collecting termcolor (from diastatic-malt->pennylane)\r\n",
            "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\r\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->pennylane) (3.4.3)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->pennylane) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->pennylane) (2.5.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->pennylane) (2024.8.30)\r\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\r\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/site-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\r\n",
            "Downloading pennylane-0.43.1-py3-none-any.whl (5.3 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/5.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m213.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading autoray-0.8.0-py3-none-any.whl (934 kB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/934.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m297.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading pennylane_lightning-0.43.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m263.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m313.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
            "Downloading autograd-1.8.0-py3-none-any.whl (51 kB)\r\n",
            "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\r\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\r\n",
            "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\r\n",
            "Downloading scipy_openblas32-0.3.30.0.8-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/8.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m252.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
            "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\r\n",
            "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\r\n",
            "Installing collected packages: appdirs, tomlkit, termcolor, scipy-openblas32, rustworkx, gast, cachetools, autoray, autograd, astunparse, diastatic-malt, pennylane-lightning, pennylane\r\n",
            "Successfully installed appdirs-1.4.4 astunparse-1.6.3 autograd-1.8.0 autoray-0.8.0 cachetools-6.2.2 diastatic-malt-2.15.2 gast-0.7.0 pennylane-0.43.1 pennylane-lightning-0.43.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.8 termcolor-3.2.0 tomlkit-0.13.3\r\n",
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Reproducibility\n",
        "SEED = 123\n",
        "\n",
        "# Imports\n",
        "import math, sys, os, json, pathlib\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp  # optional\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "print(\"pennylane version: \", qml.__version__)\n",
        "print(\"torch version:     \", torch.__version__)\n",
        "\n",
        "\n",
        "# no need to use GPU for this assignment\n",
        "device_torch = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Torch device:\", device_torch)\n",
        "\n",
        "print(\"Imports OK\")\n",
        "\n",
        "FP_TOL = 0.05  # floating point tolerance for automated tests\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/site-packages/pennylane/__init__.py:209: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.7.1 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pennylane version:  0.43.1\n",
            "torch version:      2.8.0+cu129\n",
            "Torch device: cpu\n",
            "Imports OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 0: Loading the dataset\n",
        "\n",
        "In this exercise, we will use the digits dataset from scikit-learn, but we restrict ourselves to a binary classification problem (two digits only, e.g., digits 0 and 1).\n",
        "\n",
        "Before designing the QNN, we must first:\n",
        "\n",
        "1. Load the dataset and select only two classes (0 and 1).  \n",
        "2. Split the data points into a training, validation and test sets.\n",
        "3. Apply a scaling to the input features using `StandardScaler()`, which ensures that the features have zero mean and unit variance.\n",
        "\n",
        "Remember to fit the scaler on the training data, and use the same fitted scaler on both the training, validation and test datasets!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Load full digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# All data and labels\n",
        "X_all = digits.data      # shape: (n_samples, n_features)\n",
        "y_all = digits.target  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "X.shape[0]*0.7\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "251.99999999999997"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        " (y_tr==1).sum() \n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "np.int64(129)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Data loading and preprocessing\n",
        "# TODO: Load digits dataset, pick two classes (0 and 1), train/test split, and scale features.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Load full digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# All data and labels\n",
        "X_all = digits.data      # shape: (n_samples, n_features)\n",
        "y_all = digits.target    # shape: (n_samples,)\n",
        "\n",
        "# -----YOUR CODE HERE-----\n",
        "# 1. Create a boolean mask that selects only digits 0 and 1.\n",
        "mask = (y_all == 0) | (y_all == 1)\n",
        "\n",
        "# 2. Apply the mask to X_all and y_all to get a binary dataset.\n",
        "X = X_all[mask]\n",
        "y01 = y_all[mask]\n",
        "\n",
        "# 3. Split the dataset in 70% training, 15% validation and 15% test.\n",
        "# Remember to use the seed random_state=SEED, and to stratify according to y01 and y_temp.\n",
        "X_tr, X_temp, y_tr, y_temp = train_test_split(X, y01, test_size = 0.3, stratify=y01, random_state = SEED)\n",
        "X_val, X_te, y_val, y_te = train_test_split(X_temp, y_temp, train_size = 0.5, stratify=y_temp, random_state = SEED)\n",
        "\n",
        "# 4. Make mean 0 and standard deviation 1 using StandardScaler\n",
        "scaler = StandardScaler().fit(X_tr)\n",
        "X_tr = scaler.transform(X_tr)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_te = scaler.transform(X_te)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "print(\"Shapes:\", X_tr.shape, X_val.shape, X_te.shape,\n",
        "      \" (class 1 count):\", (y_tr==1).sum(),\n",
        "      \" (class 0 count):\", (y_tr==0).sum())\n",
        "\n",
        "print(\"Test mean/std:\", X_te.mean().round(6), X_te.std().round(6))\n",
        "\n",
        "assert len(X_tr) + len(X_val) + len(X_te) == len(X)\n",
        "assert set(np.unique(y_tr)).issubset({0,1})\n",
        "assert X_tr.shape == (252, 64)\n",
        "assert X_val.shape == (54, 64)\n",
        "assert X_te.shape == (54, 64)\n",
        "assert (y_tr==1).sum() == 127\n",
        "assert (y_tr==0).sum() == 125\n",
        "assert np.isclose(X_tr.mean(), 0.0, FP_TOL, FP_TOL)\n",
        "assert np.isclose(X_te.mean(), -0.033565, FP_TOL, FP_TOL)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (252, 64) (54, 64) (54, 64)  (class 1 count): 127  (class 0 count): 125\n",
            "Test mean/std: -0.027555 0.864232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 0.1 \u2014 Short answer\n",
        "Why is feature scaling important when we use angle-embedding (e.g. RY rotations) to encode classical data into a QNN?  \n",
        "Write a concise justification (2\u20134 sentences).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: To avoid issues with wrap-around, that is when the angle is greater than or equal to 2\u03c02\\pi2\u03c0 and we run into issues with aliasing of embeddings. But if the range is too narrow instead, we lose expressivity of the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 0.2 \u2014 Short answer\n",
        "What does the parameter stratify do? (1-2 sentences).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: It makes sure the proportion of the lables in the split is the same as in the input to the stratify parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Dimensionality reduction using PCA\n",
        "\n",
        "For angle-based maps we often set the number of qubits equal (or proportional) to the feature dimension. Unfortunately, the available qubit count is often smaller than the dimensionality of the data.\n",
        "\n",
        "Here, we use PCA to reduce the data dimensionality to `n_qubits` components, so that we can encode the data on a small QNN.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Choose `n_qubits` (e.g., 4).  \n",
        "2. Fit `PCA(n_components=n_qubits)` on the training data and transform both training, validation and test sets.  \n",
        "3. Print shapes to confirm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# PCA reduction to match qubits\n",
        "# -----YOUR CODE HERE-----\n",
        "n_qubits = 4\n",
        "pca = PCA(n_components=n_qubits).fit(X_tr)\n",
        "Xtr_red = pca.transform(X_tr)\n",
        "Xval_red = pca.transform(X_val)\n",
        "Xte_red = pca.transform(X_te)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "print(\"Reduced shapes:\", Xtr_red.shape, Xval_red.shape, Xte_red.shape)\n",
        "assert Xtr_red.shape == (252, 4)\n",
        "assert Xval_red.shape == (54, 4)\n",
        "assert Xte_red.shape == (54, 4)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced shapes: (252, 4) (54, 4) (54, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 1.1 \u2014 Short answer\n",
        "Explain briefly (2\u20133 sentences) why PCA is useful in this QNN setting, and what we are trading off by compressing the features.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: Since we have a lower number of qubits, we want to reduce the dimensions while keeping the most information possible, which is essentially what PCA does. PCA captures the directions in the data space representing the most variance. Note, we could embed the data (64 dimensions) using amplitude embeddings with 6 qubits ($2^6 = 64$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: Implement a QNN feature map and variational ansatz\n",
        "\n",
        "We now implement the quantum circuit building blocks of our QNN:\n",
        "\n",
        "1. A **feature map** that encodes each PCA-reduced input into `n_qubits` using angle-embedding.  \n",
        "2. A **variational ansatz** with trainable parameters $\\theta$, which will be learned during training.\n",
        "\n",
        "We will then use these building blocks inside a PennyLane QNode.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Task 2: QNN building blocks (feature map + ansatz)\n",
        "\n",
        "# 1. Define a device with n_qubits wires in analytic mode (shots=None)\n",
        "# -----YOUR CODE HERE-----\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "def feature_map_qnn(x, scale=1.0, entangle=True):\n",
        "    \"\"\"\n",
        "    Simple angle-embedding feature map with optional entangling layer.\n",
        "\n",
        "    x: 1D array-like of length n_qubits (PCA-reduced input)\n",
        "    scale: rescaling factor for angles\n",
        "    entangle: if True, apply a CZ ring after single-qubit rotations\n",
        "    \"\"\"\n",
        "    # -----YOUR CODE HERE-----\n",
        "    wires = list(range(len(x)))\n",
        "    # 1. Apply AngleEmbedding with rotation=\"Y\" and the given scale\n",
        "    qml.AngleEmbedding(x*scale, wires=wires, rotation=\"Y\")\n",
        "    # 2. If entangle=True, apply a ring of CZ or CNOT gates\n",
        "    if entangle:\n",
        "        for i in range(len(wires)-1):\n",
        "            qml.CZ(wires[i:i+1+1])\n",
        "        qml.CZ([wires[-1], wires[0]])\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    return\n",
        "\n",
        "\n",
        "def variational_ansatz(theta):\n",
        "    \"\"\"\n",
        "    Variational circuit with trainable parameters theta.\n",
        "    \n",
        "    theta: parameters with shape (number of layers, number of wires).\n",
        "\n",
        "    Structure:\n",
        "    - For each layer:\n",
        "        - Apply RY rotations on all qubits\n",
        "        - Apply a ring of CNOTs to entangle the qubits\n",
        "    \"\"\"\n",
        "    n_layers, n_wires = theta.shape\n",
        "    # -----YOUR CODE HERE-----\n",
        "    wires = list(range(n_wires))\n",
        "    for l in range(n_layers):\n",
        "        qml.AngleEmbedding(theta[l], wires=wires, rotation=\"Y\")\n",
        "        for i in range(len(wires)-1):\n",
        "            qml.CNOT(wires[i:i+1+1])\n",
        "        qml.CNOT([wires[-1], wires[0]])\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    return\n",
        "\n",
        "#print(qml.draw(feature_map_qnn)(Xtr_red[0]))\n",
        "# dev.wires"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 2.1 \u2014 Short answer\n",
        "Explain in a few sentences how the feature map and the variational ansatz play different roles in a QNN.  \n",
        "Relate each of them to analogous components in a classical neural network.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: The feature map embeds data and is static (not trainable), while the ansatz/variational circuit has trainable parameters which are optimized for the task. This is akin to data preparation or pretrained embeddings (feature map) and the actual classical NN itself (variational circuit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 2.2 \u2014 Reflection\n",
        "Why does the absence of entangling gates in a quantum feature map or variational ansatz (or any quantum circuit in general) guarantee that the the circuit's output state can be efficiently simulated classically given a classical input such as $|0\\rangle$? (2-5 sentances)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: Without entanglement, the circuit H of dimension $2^n$ can be factorized into a product of the unitary matrices $H = \\bigotimes\\limits^n_{i=1} H_i$, where H_i are 2-by-2 unitary matrices. Which becomes $H \\ket{00\\dots0} = \\bigotimes\\limits^n_{i=1} H_i\\ket{0}$. Therefore we obtain the distrubution over output qubits as the product of independent bernoulli variables with probabilities according to the individual components $H_i\\ket{0}$, and these qunaitities are tractable to calculate on a classical computer. (To simulate one shot on the quantum computer, we can just draw n bernoulli variables)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 2.3 \u2014 Follow up\n",
        "Given this fact, can we expect any quantum advantage from a circuit composed only of single-qubit gates in a feature map or variational ansatz? Explain why or why not.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: We do not have a quantum advantage, because this can be simulated easily on a classical computer according to our answer to Question 2.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: Define the QNode and PyTorch QNN classifier\n",
        "\n",
        "We now combine the feature map and ansatz into a QNode that returns a single expectation value, and then wrap it in a PyTorch module.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Define a QNode `qnn_circuit(x, theta)`:\n",
        "   - encodes one input sample `x` using `feature_map_qnn`,\n",
        "   - applies `variational_ansatz(theta)`. Try first with scale 1 and with entangling.\n",
        "   - Return the expectation value of `PauliZ` on qubit 0. (Note that measuring PauliZ ensures that the expectation value is in [-1, 1]). Hint: Use the pennylane function `qml.expval`.\n",
        "\n",
        "2. Define a `QNNClassifier(nn.Module)` that:\n",
        "   - stores `theta` as an `nn.Parameter`,\n",
        "   - in `forward`, loops over a mini-batch of inputs,\n",
        "   - calls the QNode for each sample,\n",
        "   - maps the expectation values from [-1, 1] to probabilities in [0, 1],\n",
        "   - returns a tensor of shape `(batch_size, 1)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 3.1 \u2014 Short answer\n",
        "Why is the expectation value of a Pauli operator (e.g. Z on one qubit) a natural scalar output for a QNN used for **binary classification**?  \n",
        "How do we convert it into a probability in [0, 1] in a good way?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "Xtr_red.shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 50,
          "data": {
            "text/plain": "(252, 4)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Task 3: QNode and QNN classifier with PyTorch\n",
        "\n",
        "# Decide a theta shape, e.g. for L layers and n_qubits:\n",
        "# theta_shape = (n_layers, n_qubits)\n",
        "# You may choose a different layout if you like.\n",
        "n_layers = 2\n",
        "theta_shape = (n_layers, n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
        "def qnn_circuit(x, theta):\n",
        "    \"\"\"\n",
        "    Quantum node for a single input sample.\n",
        "\n",
        "    x: 1D tensor with n_qubits features\n",
        "    theta: trainable parameters for the ansatz\n",
        "    \"\"\"\n",
        "    # -----YOUR CODE HERE-----\n",
        "    feature_map_qnn(x)\n",
        "    variational_ansatz(theta)\n",
        "\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    \n",
        "\n",
        "def test_qnn_circuit():\n",
        "    x_sample = torch.tensor([0.1, -0.2, 0.3, 0.4], dtype=torch.float32)\n",
        "    theta_sample = torch.ones(theta_shape, dtype=torch.float32)\n",
        "    ev = qnn_circuit(x_sample, theta_sample)\n",
        "    print(\"QNN circuit output (expectation value):\", ev.item())\n",
        "    assert np.isclose(ev.item(), -0.5613872, FP_TOL, FP_TOL)\n",
        "\n",
        "test_qnn_circuit()\n",
        "\n",
        "class QNNClassifier(nn.Module):\n",
        "    def __init__(self, theta_shape):\n",
        "        super().__init__()\n",
        "        # Initialize trainable parameters theta as a PyTorch Parameter\n",
        "        self.theta = nn.Parameter(torch.randn(theta_shape) * 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch_size, n_qubits)\n",
        "        returns: probabilities in [0,1] of shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # -----YOUR CODE HERE-----\n",
        "        # 1. Ensure x has a batch dimension\n",
        "        # 2. For each sample in the batch, call qnn_circuit(sample, self.theta)\n",
        "        # 3. Stack expectation values into a tensor\n",
        "        # 4. Map from [-1,1] to [0,1]\n",
        "        # 5. Return p with shape (batch_size, 1)\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        assert len(x.shape) == 2\n",
        "        batch_size, n_qubits = x.shape\n",
        "        exps = []\n",
        "        for i in range(batch_size):\n",
        "            exps.append(qnn_circuit(x[i], self.theta))\n",
        "\n",
        "        output = (torch.stack(exps) + 1)/2\n",
        "        return output.unsqueeze(-1)\n",
        "        \n",
        "        # ---YOUR CODE ENDS HERE---\n",
        "\n",
        "\n",
        "# Instantiate model and move to Torch device\n",
        "model = QNNClassifier(theta_shape).to(device_torch)\n",
        "print(model)\n",
        "\n",
        "def test_qnn_circuit():\n",
        "    test_batch1 = torch.tensor([[0.1, -0.2, 0.3, 0.4],\n",
        "                               [0.5, 0.6, -0.7, 0.8]], dtype=torch.float32)\n",
        "    \n",
        "    test_batch2 = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32)\n",
        "        \n",
        "    assert model.forward(test_batch1).shape == (2, 1)\n",
        "    assert model.forward(test_batch2).shape == (1, 1)\n",
        "    \n",
        "    #Assert that the output is a probability\n",
        "    p = model.forward(test_batch2).item()\n",
        "    assert 0.0 <= p <= 1.0\n",
        "\n",
        "test_qnn_circuit()\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QNN circuit output (expectation value): -0.5613872548613136\n",
            "QNNClassifier()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 4: Training the QNN\n",
        "\n",
        "We now train the QNN classifier on the PCA-reduced dataset.\n",
        "\n",
        "We will use:\n",
        "\n",
        "* `nn.BCELoss` as the loss function (binary cross-entropy on probabilities),  \n",
        "* `Adam` as the optimizer,  \n",
        "* a modest number of epochs (e.g. 15\u201330).\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Wrap the training data in a `DataLoader`. Shuffle the data for the training loader but not the validation loader.\n",
        "2. Implement the standard training loop:\n",
        "   - zero gradients,\n",
        "   - forward pass,\n",
        "   - compute loss,\n",
        "   - backpropagate,\n",
        "   - optimizer step.  \n",
        "3. Track and print the training loss per epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Create DataLoaders for training and test sets using PCA-reduced features\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = TensorDataset(\n",
        "    torch.tensor(Xtr_red, dtype=torch.double),\n",
        "    torch.tensor(y_tr, dtype=torch.double).unsqueeze(1),\n",
        ")\n",
        "val_ds = TensorDataset(\n",
        "    torch.tensor(Xval_red, dtype=torch.double),\n",
        "    torch.tensor(y_val, dtype=torch.double).unsqueeze(1),\n",
        ")\n",
        "test_ds = TensorDataset(\n",
        "    torch.tensor(Xte_red, dtype=torch.double),\n",
        "    torch.tensor(y_te, dtype=torch.double).unsqueeze(1),\n",
        ")\n",
        "# -----YOUR CODE HERE-----\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "print(len(train_loader), len(val_loader), len(test_loader))\n",
        "assert len(train_loader) == 8\n",
        "assert len(val_loader) == 2\n",
        "assert len(test_loader) == 2\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 2 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Training loop\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
        "\n",
        "n_epochs = 20\n",
        "\n",
        "history = {\"loss\": [], \"val_loss\": []}\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device_torch)\n",
        "        yb = yb.to(device_torch)\n",
        "\n",
        "        # -----YOUR CODE HERE-----\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # ---YOUR CODE ENDS HERE---\n",
        "\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "    epoch_loss /= len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            val_loss += loss.item() * xb.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    history[\"loss\"].append(epoch_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    print(f\"Epoch {epoch:02d}/{n_epochs} - loss: {epoch_loss:.4f} - val_loss: {val_loss:.4f}\")\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/20 - loss: 0.6519 - val_loss: 0.6634\n",
            "Epoch 02/20 - loss: 0.6156 - val_loss: 0.6171\n",
            "Epoch 03/20 - loss: 0.5933 - val_loss: 0.5909\n",
            "Epoch 04/20 - loss: 0.5790 - val_loss: 0.5748\n",
            "Epoch 05/20 - loss: 0.5722 - val_loss: 0.5631\n",
            "Epoch 06/20 - loss: 0.5676 - val_loss: 0.5547\n",
            "Epoch 07/20 - loss: 0.5635 - val_loss: 0.5509\n",
            "Epoch 08/20 - loss: 0.5604 - val_loss: 0.5488\n",
            "Epoch 09/20 - loss: 0.5573 - val_loss: 0.5486\n",
            "Epoch 10/20 - loss: 0.5552 - val_loss: 0.5491\n",
            "Epoch 11/20 - loss: 0.5530 - val_loss: 0.5510\n",
            "Epoch 12/20 - loss: 0.5504 - val_loss: 0.5528\n",
            "Epoch 13/20 - loss: 0.5484 - val_loss: 0.5515\n",
            "Epoch 14/20 - loss: 0.5473 - val_loss: 0.5517\n",
            "Epoch 15/20 - loss: 0.5463 - val_loss: 0.5525\n",
            "Epoch 16/20 - loss: 0.5455 - val_loss: 0.5533\n",
            "Epoch 17/20 - loss: 0.5456 - val_loss: 0.5554\n",
            "Epoch 18/20 - loss: 0.5451 - val_loss: 0.5546\n",
            "Epoch 19/20 - loss: 0.5460 - val_loss: 0.5555\n",
            "Epoch 20/20 - loss: 0.5450 - val_loss: 0.5537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Plot training loss\n",
        "\n",
        "plt.figure(figsize=(4, 3))\n",
        "plt.plot(history[\"loss\"], marker='o')\n",
        "plt.plot(history[\"val_loss\"], marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Train loss (BCE)\")\n",
        "plt.title(\"QNN training loss\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": "<Figure size 400x300 with 1 Axes>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 4.1 \u2014 Short answer\n",
        "Comment briefly on the training and validation curves you obtained.\n",
        "Does the loss decrease steadily? Do you see signs of underfitting or overfitting?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: The training loss keeps decreasing steadily and starts to plain out around epoch 20, while the val loss as reaches its minimum after 5-10 epochs. This means we may be seeing a slight overfit after 10 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5: Evaluation on the test set\n",
        "\n",
        "Finally, we evaluate the trained QNN on the **held-out test set**.\n",
        "\n",
        "1. Compute predicted probabilities on the test set.  \n",
        "2. Convert probabilities to class predictions using a 0.5 threshold.  \n",
        "3. Compute and print:\n",
        "   - test accuracy,  \n",
        "   - confusion matrix,  \n",
        "   - classification report (optional).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "model.eval()\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device_torch)\n",
        "        yb = yb.to(device_torch)\n",
        "\n",
        "        probs = model(xb)  # (batch_size, 1)\n",
        "        all_probs.append(probs.cpu())\n",
        "        all_labels.append(yb.cpu())\n",
        "\n",
        "all_probs = torch.cat(all_probs, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "# Convert probabilities to class predictions\n",
        "y_pred = (all_probs >= 0.5).int()\n",
        "y_true = all_labels.int()\n",
        "\n",
        "acc = accuracy_score(y_true.numpy(), y_pred.numpy())\n",
        "cm = confusion_matrix(y_true.numpy(), y_pred.numpy())\n",
        "\n",
        "print(f\"Test accuracy: {acc:.3f}\")\n",
        "print(\"Confusion matrix [[TN, FP], [FN, TP]]:\")\n",
        "print(cm)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_true.numpy(), y_pred.numpy()))\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.833\n",
            "Confusion matrix [[TN, FP], [FN, TP]]:\n",
            "[[21  6]\n",
            " [ 3 24]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": "<Figure size 640x480 with 2 Axes>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.78      0.82        27\n",
            "           1       0.80      0.89      0.84        27\n",
            "\n",
            "    accuracy                           0.83        54\n",
            "   macro avg       0.84      0.83      0.83        54\n",
            "weighted avg       0.84      0.83      0.83        54\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 5.1 \u2014 Short answer\n",
        "Does the confusion matrix show any interesting asymmetry (e.g., more false positives than false negatives)?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: We have more false positives (6) than false negatives (3), that is double the amount of FP compared to FN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bonus: Gradient inspection and barren plateaus\n",
        "\n",
        "In this bonus exercise, you will perform a more detailed gradient inspection on your QNN to connect with the concept of barren plateaus.\n",
        "\n",
        "We will focus on three aspects:\n",
        "\n",
        "1. Gradient norms at initialization for different circuit depths (`n_layers`).  \n",
        "2. Gradient norms during training (how they evolve across epochs).  \n",
        "3. Distribution of per-parameter gradients at a given point in training.\n",
        "\n",
        "\n",
        "You may reuse your QNN architecture from the main tasks (feature map, ansatz, QNode, and `QNNClassifier`), or define a simplified variant dedicated to this analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus 0 \u2014 Helper: single-batch gradient computation\n",
        "\n",
        "We start by writing a helper function that:\n",
        "\n",
        "- Takes a model, a data loader, and a loss function.  \n",
        "- Grabs one mini-batch from the loader.  \n",
        "- Computes the loss and then the gradient of the loss w.r.t. `model.theta`.  \n",
        "- Returns the L2 norm of that gradient, divided by the square root of the number of parameters (and optionally the gradient tensor itself).\n",
        "\n",
        "We will reuse this helper in the following subtasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Bonus A.0 \u2014 Helper: compute gradient norm for one batch\n",
        "\n",
        "# Normalized gradient norm computation\n",
        "def gradient_norm_for_one_batch(model, data_loader, criterion, device=device_torch):\n",
        "    \"\"\"\n",
        "    Compute the L2 norm of the gradient of the loss w.r.t. model.theta\n",
        "    using a single batch from data_loader.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    # Take the first batch only\n",
        "    xb, yb = next(iter(data_loader))\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    # Zero existing gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    preds = model(xb)\n",
        "    loss = criterion(preds, yb)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Extract gradient of theta\n",
        "    grad_theta = model.theta.grad  # tensor with same shape as theta\n",
        "    grad_norm = torch.norm(grad_theta).item() / math.sqrt(grad_theta.numel())\n",
        "\n",
        "    return grad_norm, grad_theta.detach().cpu(), loss.item()\n",
        "\n",
        "# Quick sanity check (optional, only if model is already defined)\n",
        "criterion = nn.BCELoss()\n",
        "g_norm, g_tensor, loss_val = gradient_norm_for_one_batch(model, train_loader, criterion)\n",
        "print(\"Gradient norm (one batch):\", g_norm, \"Loss:\", loss_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus 1 \u2014 Gradient norms at initialization vs circuit depth\n",
        "\n",
        "In this part, you will study how the gradient norm at random initialization depends on the circuit depth `n_layers`.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Choose a list of depths, e.g. `depth_list = [1, 2, 3]` (you may add more if runtime allows).  \n",
        "2. For each depth:\n",
        "   - Rebuild a fresh QNN (same architecture as in the main task, but with that `n_layers`).  \n",
        "   - Randomly initialize the parameters (the default PyTorch initialization is fine).  \n",
        "   - Compute the gradient norm on a single training batch using `gradient_norm_for_one_batch`.  \n",
        "3. Store the gradient norms and print them (and optionally plot them).  \n",
        "4. Comment on whether deeper circuits tend to show smaller gradient norms at initialization.\n",
        "\n",
        "You can reuse your `QNNClassifier` class, but you will need to make `n_layers` configurable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Bonus A.1 \u2014 Gradient norm vs depth at initialization\n",
        "\n",
        "# Make sure you have a way to construct a QNNClassifier with a given n_layers.\n",
        "# For example, you can wrap the model creation in a small factory function.\n",
        "\n",
        "def make_qnn_model(n_layers, n_qubits):\n",
        "    \"\"\"\n",
        "    Factory for QNNClassifier with given depth n_layers and n_qubits.\n",
        "    Adjust theta_shape and ansatz accordingly.\n",
        "    \"\"\"\n",
        "    # Example: theta_shape = (n_layers, n_qubits)\n",
        "    theta_shape = (n_layers, n_qubits)\n",
        "    model = QNNClassifier(theta_shape).to(device_torch)\n",
        "    return model\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "depth_list = [1, 2, 3, 4, 5, 6, 7]  # you can add 4, 5, ... if time allows\n",
        "grad_norms_depth = []\n",
        "\n",
        "for depth in depth_list:\n",
        "# -----YOUR CODE HERE-----\n",
        "\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "# Optional: plot gradient norms vs depth\n",
        "plt.figure(figsize=(4, 3))\n",
        "plt.plot(depth_list, grad_norms_depth, marker=\"o\")\n",
        "plt.xlabel(\"n_layers (circuit depth)\")\n",
        "plt.ylabel(\"Gradient norm at init\")\n",
        "plt.title(\"Gradient norm vs depth at initialization\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 1.1 \u2014 Interpretation\n",
        "Comment on the gradient norms vs depth that you observed.\n",
        "\n",
        "- Do deeper circuits tend to show smaller gradient norms at random initialization?  \n",
        "- How does this relate qualitatively to the idea of barren plateaus (even in this small setup)?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus 2 \u2014 Gradient norm across training epochs\n",
        "\n",
        "We now study how the gradient norm evolves during training for a fixed circuit depth.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Pick one depth (e.g. `n_layers = 2`).  \n",
        "2. Train the QNN for a small number of epochs (e.g. 10) as in the main assignment.  \n",
        "3. At each epoch:\n",
        "   - After the weight update, compute the gradient norm on one batch using `gradient_norm_for_one_batch`.  \n",
        "   - Store this value in a list `grad_norms_epochs`.  \n",
        "4. Plot gradient norm vs epoch, and compare this with the training loss curve.  \n",
        "5. Comment on whether gradients tend to shrink as training progresses.\n",
        "\n",
        "> Hint: You can reuse your training loop, but add a call to `gradient_norm_for_one_batch` at the end of each epoch, using the current model parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Bonus A.2 \u2014 Gradient norm vs epoch for a fixed depth\n",
        "\n",
        "# Choose a depth\n",
        "depth_for_training = 2\n",
        "model_train = make_qnn_model(depth_for_training, n_qubits)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model_train.parameters(), lr=0.02)\n",
        "\n",
        "n_epochs_bonus = 10\n",
        "losses_epochs = []\n",
        "grad_norms_epochs = []\n",
        "\n",
        "for epoch in range(1, n_epochs_bonus + 1):\n",
        "    model_train.train()\n",
        "    epoch_loss = 0.0\n",
        "    # -----YOUR CODE HERE-----\n",
        "\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    grad_norms_epochs.append(g_norm)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}/{n_epochs_bonus} - Loss: {epoch_loss:.4f}, Grad norm: {g_norm:.4e}\")\n",
        "\n",
        "# Plot loss and gradient norm vs epoch\n",
        "fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Train loss (BCE)\", color=\"C0\")\n",
        "ax1.plot(range(1, n_epochs_bonus+1), losses_epochs, marker=\"o\", color=\"C0\", label=\"Loss\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"C0\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Gradient norm\", color=\"C1\")\n",
        "ax2.plot(range(1, n_epochs_bonus+1), grad_norms_epochs, marker=\"s\", color=\"C1\", label=\"Grad norm\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"C1\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.title(f\"Depth = {depth_for_training}: Loss and gradient norm vs epoch\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 2.1 \u2014 Observation\n",
        "Describe how the gradient norm changes during training: Does it decrease, stay roughly constant, or fluctuate?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 2.2 \u2014 Interpretation\n",
        "How does this relate to the changes in the training loss?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 2.3 \u2014 Interpretation\n",
        "In general, if our QNN suffered from a barren plateau, how would that affect the gradient norm?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus 3 \u2014 Distribution of per-parameter gradients\n",
        "\n",
        "So far, we have focused on the normalized L2 norm of the gradient.  \n",
        "Now, we take a closer look at the distribution of individual gradient components.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Using a model (for some depth), compute the gradient of the loss w.r.t. `model.theta` on one batch.  \n",
        "2. Extract all gradient entries as a 1D array and plot a histogram of their values.  \n",
        "Do step 1 and 2:\n",
        "   - once at initialization, and  \n",
        "   - once after training, and compare the two histograms.\n",
        "\n",
        "This gives a sense of whether gradients are broadly distributed or tend to concentrate tightly around zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Bonus A.3 \u2014 Gradient distribution (histogram)\n",
        "\n",
        "def gradient_histogram(model, data_loader, criterion, device=device_torch, title=\"\"):\n",
        "    \"\"\"\n",
        "    Compute gradients on one batch and plot a histogram of gradient values.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    xb, yb = next(iter(data_loader))\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "    \n",
        "    # -----YOUR CODE HERE-----\n",
        "\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "\n",
        "    grad_vals = model.theta.grad.detach().cpu().numpy().ravel()\n",
        "\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    plt.hist(grad_vals, bins=40, alpha=0.8)\n",
        "    plt.xlabel(\"Gradient value\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(title)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"{title} \u2014 mean grad: {grad_vals.mean():.2e}, std grad: {grad_vals.std():.2e}\")\n",
        "    return grad_vals\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "depth = 2\n",
        "# Example: histogram at initialization for specific depth\n",
        "model_init = make_qnn_model(depth, n_qubits)\n",
        "print(\"Gradient distribution at initialization (depth=2):\")\n",
        "_ = gradient_histogram(model_init, train_loader, criterion, device=device_torch,\n",
        "                       title=\"Gradients at init (depth=2)\")\n",
        "\n",
        "# Example: histogram after training for depth=2 (reuse model_train from Bonus A.2 if available)\n",
        "try:\n",
        "    print(\"Gradient distribution after training (depth=2):\")\n",
        "    _ = gradient_histogram(model_train, train_loader, criterion, device=device_torch,\n",
        "                           title=\"Gradients after training (depth=2)\")\n",
        "except NameError:\n",
        "    print(\"model_train not found. Run Bonus A.2 first or train a model for this analysis.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the gradient histograms you obtained:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 3.1 \u2014 Observation\n",
        "Are gradients broadly distributed or sharply concentrated near zero?  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 3.2 \u2014 Short answer\n",
        "Does training make them more or less concentrated?  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 3.3 \u2014 Reflection\n",
        "How would a highly concentrated distribution of very small gradients relate to barren plateaus?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feedback to us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Optional question\n",
        "Was there any part of the tasks where you struggled for some \"unnecessary\" reason? (Errors in the notebook, bad instructions etc.)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your optional answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Disclosure of AI Usage (Mandatory)\n",
        "Fill in this part disclosing any AI usage before submitting the assignment by describing your use of LLMs or other AI-based tools in this assignment.\n",
        "\n",
        "For each task, we ask you to provide information about:\n",
        "- **Tools/models used**.\n",
        "- **Per\u2011task usage**: for each task, a brief summary of what the tool was used for.\n",
        "- **Prompts/transcripts**: main prompts or a summary of interactions (a link or screenshot is acceptable if long).\n",
        "- **Validation**: how you checked and verified the correctness of AI-generated outputs (tests run, docs consulted, comparisons, plots etc.).\n",
        "\n",
        "Disclosure:\n",
        "- **Task 0**: [...describe your use...]\n",
        "\n",
        "- **Task 1**: [...describe your use...]\n",
        "\n",
        "- **Task 2**: [...describe your use...]\n",
        "\n",
        "- **Task 3**: [...describe your use...]\n",
        "\n",
        "- **Task 4**: [...describe your use...]\n",
        "\n",
        "- **Task 5**: [...describe your use...]\n",
        "\n",
        "- **Bonus 1**: [...describe your use...]\n",
        "\n",
        "- **Bonus 2**: [...describe your use...]\n",
        "\n",
        "- **Bonus 3**: [...describe your use...]\n",
        "\n",
        "If you did not use any AI tools for a given exercise, specify this by writing \"None\". If you did not complete the bonus exercises, you can leave those fields empty."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}