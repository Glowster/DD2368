{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Assignment Module 2: Quantum\u2011Kernel SVM (QSVM)\n",
        "\n",
        "This assignment focuses on **quantum\u2011kernel SVM (QSVM)** using a *Pennylane* implementation of the **inverse\u2011circuit overlap test**.\n",
        "\n",
        "To complete the assignment, you will implement missing code (marked **YOUR CODE HERE**) and answer a set of short theoretical questions.\n",
        "\n",
        "## Preparation\n",
        "\n",
        "* Look at the [notebook](https://canvas.kth.se/courses/57561/files/9801323?module_item_id=1339648) on Quantum SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use of generative AI tools\n",
        "\n",
        "You may use AI-based tools (e.g., ChatGPT, GitHub Copilot, Claude, Gemini, DeepSeek, ...) for brainstorming, refactoring, coding assistance, plotting, or editing.\n",
        "\n",
        "This is allowed with disclosure. LLMs are a great tool, but you have to make sure to grasp the contents of the course!\n",
        "\n",
        "**Make sure to fill in the mandatory AI-disclosure in the notebook before submitting!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparatory code\n",
        "\n",
        "Run this to import the modules we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "!pip install pennylane\n",
        "!pip install jax~=0.6.0 jaxlib~=0.6.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\r\n",
            "  Downloading pennylane-0.43.1-py3-none-any.whl.metadata (11 kB)\r\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/site-packages (from pennylane) (1.16.1)\r\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from pennylane) (3.3)\r\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\r\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\r\n",
            "Collecting autograd (from pennylane)\r\n",
            "  Downloading autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\r\n",
            "Collecting appdirs (from pennylane)\r\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\r\n",
            "Collecting autoray==0.8.0 (from pennylane)\r\n",
            "  Downloading autoray-0.8.0-py3-none-any.whl.metadata (6.1 kB)\r\n",
            "Collecting cachetools (from pennylane)\r\n",
            "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\r\n",
            "Collecting pennylane-lightning>=0.43 (from pennylane)\r\n",
            "  Downloading pennylane_lightning-0.43.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from pennylane) (2.32.5)\r\n",
            "Collecting tomlkit (from pennylane)\r\n",
            "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\r\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/site-packages (from pennylane) (4.12.2)\r\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from pennylane) (25.0)\r\n",
            "Collecting diastatic-malt (from pennylane)\r\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from pennylane) (2.1.2)\r\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.43->pennylane)\r\n",
            "  Downloading scipy_openblas32-0.3.30.0.8-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\r\n",
            "Collecting astunparse (from diastatic-malt->pennylane)\r\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\r\n",
            "Collecting gast (from diastatic-malt->pennylane)\r\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\r\n",
            "Collecting termcolor (from diastatic-malt->pennylane)\r\n",
            "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\r\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->pennylane) (3.4.3)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->pennylane) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->pennylane) (2.5.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->pennylane) (2024.8.30)\r\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\r\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/site-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\r\n",
            "Downloading pennylane-0.43.1-py3-none-any.whl (5.3 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/5.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.3/5.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading autoray-0.8.0-py3-none-any.whl (934 kB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/934.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m393.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading pennylane_lightning-0.43.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m301.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
            "Downloading autograd-1.8.0-py3-none-any.whl (51 kB)\r\n",
            "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\r\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\r\n",
            "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\r\n",
            "Downloading scipy_openblas32-0.3.30.0.8-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/8.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m218.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\r\n",
            "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\r\n",
            "Installing collected packages: appdirs, tomlkit, termcolor, scipy-openblas32, rustworkx, gast, cachetools, autoray, autograd, astunparse, diastatic-malt, pennylane-lightning, pennylane\r\n",
            "Successfully installed appdirs-1.4.4 astunparse-1.6.3 autograd-1.8.0 autoray-0.8.0 cachetools-6.2.2 diastatic-malt-2.15.2 gast-0.6.0 pennylane-0.43.1 pennylane-lightning-0.43.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.8 termcolor-3.2.0 tomlkit-0.13.3\r\n",
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
            "Collecting jax~=0.6.0\r\n",
            "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\r\n",
            "Collecting jaxlib~=0.6.0\r\n",
            "  Downloading jaxlib-0.6.2-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.3 kB)\r\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/site-packages (from jax~=0.6.0) (0.5.3)\r\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/site-packages (from jax~=0.6.0) (2.1.2)\r\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/site-packages (from jax~=0.6.0) (3.4.0)\r\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/site-packages (from jax~=0.6.0) (1.16.1)\r\n",
            "Downloading jax-0.6.2-py3-none-any.whl (2.7 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m138.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading jaxlib-0.6.2-cp312-cp312-manylinux2014_x86_64.whl (89.9 MB)\r\n",
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/89.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m34.3/89.9 MB\u001b[0m \u001b[31m172.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m71.8/89.9 MB\u001b[0m \u001b[31m179.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m89.9/89.9 MB\u001b[0m \u001b[31m178.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hInstalling collected packages: jaxlib, jax\r\n",
            "  Attempting uninstall: jaxlib\r\n",
            "    Found existing installation: jaxlib 0.7.1\r\n",
            "    Uninstalling jaxlib-0.7.1:\r\n",
            "      Successfully uninstalled jaxlib-0.7.1\r\n",
            "  Attempting uninstall: jax\r\n",
            "    Found existing installation: jax 0.7.1\r\n",
            "    Uninstalling jax-0.7.1:\r\n",
            "      Successfully uninstalled jax-0.7.1\r\n",
            "Successfully installed jax-0.6.2 jaxlib-0.6.2\r\n",
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Reproducibility\n",
        "SEED = 123\n",
        "\n",
        "# Imports\n",
        "import math, sys, os, json, pathlib\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp  # optional\n",
        "\n",
        "print(\"pennylane versino: \", qml.__version__)\n",
        "\n",
        "print(\"Imports OK\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/site-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.1 is installed, but it is not compatible with the installed jaxlib version 0.6.2, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pennylane versino:  0.43.1\n",
            "Imports OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 0: Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this exercise, we will use the breast cancer dataset. Before designing the QSVM, we must first load the dataset, and split the datapoints into a training and test set. Furthermore, we will apply a scaling to the input features using the `StandardScaler()`, which ensures that the features have zero mean and unity variance. Remember to fit the scaler on the training data, and using the same fitted scaler on both the training and test datasets!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Data loading and preprocessing\n",
        "# TODO: Load Breast Cancer dataset, map labels to {-1,+1}, train/test split, and scale features.\n",
        "\n",
        "data = load_breast_cancer()\n",
        "# -----YOUR CODE HERE-----\n",
        "# Load dataset\n",
        "X = data['data']\n",
        "y01 = data['target']\n",
        "\n",
        "# Remap binary labels from 0/1 to -1/+1\n",
        "y = np.where(y01==1, +1, -1)\n",
        "\n",
        "# Split in train and test datasets\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y)\n",
        "\n",
        "# Make mean 0 and standard diviation 1\n",
        "scaler = StandardScaler()\n",
        "X_tr = scaler.fit_transform(X_tr)\n",
        "X_te = scaler.transform(X_te)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "print(\"Shapes:\", X_tr.shape, X_te.shape, \" (+1 count):\", (y_tr==+1).sum(), \" (-1 count):\", (y_tr==-1).sum())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (426, 30) (143, 30)  (+1 count): 275  (-1 count): 151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 0.1 \u2014 Short answer\n",
        "Why is feature scaling important for angle\u2011embedding feature maps? Write a concise justification (2\u20134 sentences).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: To avoid issues with wrap-around, that is when the angle is greater than or equal to $2\\pi$ and we run into issues with aliasing of embeddings. But if the range is too narrow instead, we lose expressivity of the embeddings.\n",
        "\n",
        "\n",
        "\n",
        "## **TODO!!!!!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Task 1: Dimensionality reduction using PCA\n",
        "For angle\u2011based maps we set the number of qubits equal to the feature dimension. Unfortunately, the available qubit count is often smaller than the dimensionality of the data. Here, we use PCA to reduce the data dimensionality to `n_qubits` components. Remember to fit the PCA transform on the training data, and using the same fitted scaler on both the training and test datasets!\n",
        "\n",
        "**Instructions:**\n",
        "1. Choose `n_qubits` (e.g., 8).\n",
        "2. Fit `PCA(n_components=n_qubits)` on the training data and transform train/test.\n",
        "3. Print shapes to confirm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# PCA reduction to match qubits\n",
        "# -----YOUR CODE HERE-----\n",
        "n_qubits = 8\n",
        "pca = PCA(n_components=n_qubits)\n",
        "Xtr_red = pca.fit_transform(X_tr)\n",
        "Xte_red = pca.transform(X_te)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "print(\"Reduced shapes:\", Xtr_red.shape, Xte_red.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced shapes: (426, 8) (143, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 1.1 \u2014 Short answer\n",
        "What are the trade\u2011offs in choosing a small vs a large `n_qubits` for angle\u2011embedding maps (consider noise/depth, and dataset size,...)?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: With more qubits we get more expressivity but we are also more exposed to noise as we use more features. Additionally more qubits means a higher requirement on memory and processing, which might not work depending on hardware contstraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: Implement an angle\u2011embedding feature map\n",
        "Implement a simple angle\u2011embedding feature map followed by a ring of $CZ$ entanglers.\n",
        "\n",
        "**Instructions:**\n",
        "1. Define a device `default.qubit` with `n_qubits` wires and analytic mode (`shots=None`).\n",
        "2. Implement `feature_map_angle(x, scale=1.0, entangle=True)` that applies `AngleEmbedding(..., rotation=\"Y\")` and a CZ ring when `entangle=True`.\n",
        "3. Do **not** return anything; this is a template used inside a QNode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Device and feature map\n",
        "\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None, seed=SEED)\n",
        "\n",
        "def feature_map_angle(x, wires=None, scale=1.0, entangle=True):\n",
        "    # -----YOUR CODE HERE-----\n",
        "    qml.AngleEmbedding(x, wires=wires, rotation=\"Y\")\n",
        "    if entangle:\n",
        "        \n",
        "        # **** ------ *****\n",
        "        from itertools import combinations\n",
        "        # **** ------ *****\n",
        "        \n",
        "        for i in range(len(wires)-1):\n",
        "            qml.CZ(wires[i:i+1+1])\n",
        "        qml.CZ([wires[-1], wires[0]])\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    return None\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 2.1: Short answer\n",
        "Explain (2\u20134 sentences) how the entangling layer can change the induced kernel compared to a no\u2011entanglement feature map.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: If we don't have entanglement the probability of measuring the qubits will be independent given the features (as there are no entanglement connection), so this is might aswell be implemented on a classical computer. When we entangle the states our quantum kernel can be much more expressive, and have more complicated dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: Inverse\u2011circuit (overlap) kernel QNode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We estimate $k(x,z) = |\\langle \\phi(z) | \\phi(x) \\rangle|^2$ by the inverse\u2011circuit method.\n",
        "\n",
        "**Instructions:**\n",
        "Implement the function `all_probabilities(x, z, scale=1.0, entangle=True)` as a QNode that\n",
        "   1. Encodes `x` with `feature_map_angle`\n",
        "   2. Applies the adjoint encoding of `z`,\n",
        "   3. Returns the output probabilities corresponding to all possible bitstring outcomes using `qml.probs()`.\n",
        "\n",
        "The kernel $k(x,z)$ equals the probability of measuring zero on all qubits, which we denote as $\\Pr(0^n)$. We must therefore extract the probability of this specific outcome. We provide a helper function `overlap_probability()` that extracts $\\Pr(0^n)$.\n",
        "\n",
        "Call `overlap_probability()` on a small subset of your training data points to confirm outputs lie in the range $[0,1]$!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# TODO: QNode returning Pr(0^n)\n",
        "@qml.qnode(dev)\n",
        "def all_bitstring_probabilities(x, z, scale=1.0, entangle=True):\n",
        "    # -----YOUR CODE HERE-----\n",
        "    feature_map_angle(x, wires=list(range(len(x))), scale = scale, entangle=entangle)\n",
        "    qml.adjoint(feature_map_angle)(z, wires=list(range(len(z))), scale=scale, entangle=entangle)\n",
        "    probs = qml.probs()\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    return probs\n",
        "\n",
        "def overlap_probability(x, z, scale=1.0, entangle=True):\n",
        "    return all_bitstring_probabilities(x, z, scale=scale, entangle=entangle)[0]\n",
        "\n",
        "# Quick sanity check for x_5\n",
        "print(\"Overlap:\", overlap_probability(Xtr_red[5], Xtr_red[5]))\n",
        "# print(qml.draw(all_bitstring_probabilities)(Xtr_red[5], Xtr_red[5]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overlap: 0.9999999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 3.1 \u2014 Derivation\n",
        "Show that $\\Pr(0^n)$ equals the fidelity kernel value $k(x,z)=|\\langle \\phi(z) | \\phi(x) \\rangle|$, where $\\Pr(0^n)$ denotes the probability of measuring 0 on all qubits in the circuit above. Provide a short derivation using Dirac notation (2\u20138 lines).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]\n",
        "\n",
        "\n",
        "## **TODO!!!!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 4: Build a Gram matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement `quantum_kernel(A, B, ...)` that returns a matrix $K$ with entries $K_{ij}=k(A_i,B_j)$.\n",
        "\n",
        "**Instructions:**\n",
        "1. Loop over rows of `A` and `B` and call `overlap_probability` in order to construct the Gram matrix.\n",
        "2. Time your function on a small subset (e.g., 40\u00d740) and comment on the $(O(n^2)$ cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Gram matrix builder\n",
        "def quantum_kernel(A, B, scale=1.0, entangle=True):\n",
        "    # -----YOUR CODE HERE-----\n",
        "    K = np.array([[overlap_probability(a,b) for b in B] for a in A])\n",
        "    \n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    return K\n",
        "\n",
        "# mini benchmark\n",
        "Amini = Xtr_red[:40]; Bmini = Xtr_red[:40]\n",
        "Kmini = quantum_kernel(Amini, Bmini)\n",
        "print(\"Kmini shape:\", Kmini.shape, \"  min/max:\", Kmini.min(), Kmini.max()-0.0000000000000009)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kmini shape: (40, 40)   min/max: 8.255183615837602e-12 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 4.1: Symmetry and jitter\n",
        "Why do we not need to symmetrize and add jitter now that we run on a simulator?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: Since we have no true noise and exact probabilities calculated classically, which we would not get using a quantum computer. The simulated device is running with shots=None, and so we do not get the noise that normally can cause tiny negative eigenvalues (non-PSD behaviour)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5: Train a QSVM with a precomputed kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train an SVM on the **precomputed** Gram matrix.\n",
        "\n",
        "**Instructions.**\n",
        "1. Compute the Gram matrix of `Xtr_red` and `Xtr_red`.\n",
        "2. Fit `SVC(kernel=\"precomputed\", C=...)` on `(K_tr, y_tr)`.\n",
        "3. Compute the Gram matrix of the `Xtr_red` and `Xtr_red`. Predict on test data, report accuracy and confusion matrix.\n",
        "4. Briefly comment on your results (1\u20133 sentences).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# -----YOUR CODE HERE-----\n",
        "# Create the Gram matrix for the training data.\n",
        "# Note that this might take around 20-40 min to run.\n",
        "K_tr = quantum_kernel(Xtr_red, Xtr_red)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "# We save the matrix for further use\n",
        "np.savetxt(\"K_tr.mpy\", K_tr)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the Gram matrix to train the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "K_tr = np.loadtxt(\"K_tr.mpy\")\n",
        "\n",
        "C = 1.0  # Hyper parameter you may tune\n",
        "clf = SVC(kernel=\"precomputed\", C=C)\n",
        "\n",
        "# -----YOUR CODE HERE-----\n",
        "# Fit the classifier to the training data\n",
        "SVM = clf.fit(K_tr.T, y_tr.T)\n",
        "# ---YOUR CODE ENDS HERE---\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the Gram matrix for all pairs of training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# -----YOUR CODE HERE-----\n",
        "X_concat = np.concatenate((Xtr_red, Xte_red))\n",
        "K_te_tr = quantum_kernel(X_concat, X_concat)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "np.savetxt(\"K_te_tr.mpy\", K_te_tr)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the classifier and the Gram matrix of the training and test data to predict labels for the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "K_te_tr = np.loadtxt(\"K_te_tr.mpy\")\n",
        "\n",
        "# -----YOUR CODE HERE-----\n",
        "y_pred = SVM.predict(K_te_tr)\n",
        "acc = accuracy_score(y_pred, np.concaty_te)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "print(f\"QSVM test accuracy: {acc:.4f}\")\n",
        "cm = confusion_matrix(y_te, y_pred, labels=[-1,+1])\n",
        "ConfusionMatrixDisplay(cm, display_labels=[\"-1\",\"+1\"]).plot(values_format=\"d\"); plt.show()\n",
        "print(classification_report(y_te, y_pred, target_names=[\"neg(-1)\",\"pos(+1)\"]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 569 features, but SVC is expecting 426 features as input.",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m K_te_tr = np.loadtxt(\u001b[33m\"\u001b[39m\u001b[33mK_te_tr.mpy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----YOUR CODE HERE-----\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y_pred = \u001b[43mSVM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK_te_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m acc = accuracy_score(y_pred, np.concaty_te)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# ---YOUR CODE ENDS HERE---\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/sklearn/svm/_base.py:822\u001b[39m, in \u001b[36mBaseSVC.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    820\u001b[39m     y = np.argmax(\u001b[38;5;28mself\u001b[39m.decision_function(X), axis=\u001b[32m1\u001b[39m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     y = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes_.take(np.asarray(y, dtype=np.intp))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/sklearn/svm/_base.py:436\u001b[39m, in \u001b[36mBaseLibSVM.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    421\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[32m    422\u001b[39m \n\u001b[32m    423\u001b[39m \u001b[33;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m \u001b[33;03m        The predicted values.\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m     predict = \u001b[38;5;28mself\u001b[39m._sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dense_predict\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/sklearn/svm/_base.py:614\u001b[39m, in \u001b[36mBaseLibSVM._validate_for_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    611\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.kernel):\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp.issparse(X):\n\u001b[32m    625\u001b[39m     X = sp.csr_matrix(X)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/sklearn/utils/validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/sklearn/utils/validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: X has 569 features, but SVC is expecting 426 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 5.1: Short answer\n",
        "Where do labels $(\\pm 1)$ enter the SVM training when using a precomputed kernel? Answer in 2\u20134 sentences.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Task 6: Classical baseline (RBF SVM)\n",
        "The code below trains a conventional SVM with an RBF kernel on the same PCA\u2011reduced features. Compare accuracy and qualitative behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "rbf = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=SEED)\n",
        "rbf.fit(Xtr_red, y_tr)\n",
        "rbf_pred = rbf.predict(Xte_red)\n",
        "print(\"RBF SVM accuracy:\", accuracy_score(y_te, rbf_pred))\n",
        "cm = confusion_matrix(y_te, rbf_pred, labels=[-1,+1])\n",
        "ConfusionMatrixDisplay(cm, display_labels=[\"-1\",\"+1\"]).plot(values_format=\"d\"); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 6.1: Short answer\n",
        "How does the result differ between the QSVM and the classical SVM? Answer in 2\u20134 sentences.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Bonus A: Gram\u2011matrix spectral analysis\n",
        "Perform a basic spectral analysis to diagnose kernel quality and potential concentration.\n",
        "\n",
        "**Instructions:**\n",
        "1. Compute eigenvalues of $K$ (and of the centered kernel $K_c = H K H$ with $H=I-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$).\n",
        "2. Plot scree diagrams (eigenvalues in descending order).\n",
        "3. Compute effective rank $r_\\mathrm{eff} = (\\sum \\lambda_i)^2 / \\sum \\lambda_i^2$.\n",
        "4. Plot histograms of same\u2011class vs different\u2011class similarities.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Spectral analysis of K\n",
        "# -----YOUR CODE HERE-----\n",
        "\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "plt.figure(); plt.plot(eigs, marker='o'); plt.title(f\"Spectrum of K (eff. rank \u2248 {eff_rank:.2f})\")\n",
        "plt.xlabel(\"index\"); plt.ylabel(\"eigenvalue\"); plt.show()\n",
        "\n",
        "plt.figure(); plt.plot(eigs_c, marker='o'); plt.title(\"Spectrum of centered K_c\")\n",
        "plt.xlabel(\"index\"); plt.ylabel(\"eigenvalue\"); plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# -----YOUR CODE HERE-----\n",
        "# Similarity histogram: same vs diff classes\n",
        "\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(same, bins=40, alpha=0.6, density=False, label=\"same-class\")\n",
        "plt.hist(diff, bins=40, alpha=0.6, density=False, label=\"diff-class\")\n",
        "plt.title(\"Kernel similarities: same vs different class\"); plt.legend(); plt.show()\n",
        "\n",
        "print(\"Top-5 eigenvalues:\", eigs[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus A.1: Interpretation\n",
        "Comment on the spectra you obtained. Do you see signs of kernel concentration (near\u2011identity or rank\u20111 structure)? How does the centered spectrum differ?\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Bonus B: Feature-map variations\n",
        "Evaluate the impact of modifying the feature map (e.g., adding depth or changing rotation axis).\n",
        "\n",
        "**Instructions:**\n",
        "1. Implement an alternative map (e.g., two angle+CZ layers or `rotation=\"X\"`).\n",
        "2. Rebuild $K_{tr}$, $K_{te}$, retrain the SVM, and report test accuracy.\n",
        "3. Compare spectra as in Bonus A. Summarize observations (3-6 sentences).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Alternative feature map (example: deeper Y-embedding with two layers)\n",
        "# -----YOUR CODE HERE-----\n",
        "\n",
        "# ---YOUR CODE ENDS HERE---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus B.1: Wrap\u2011up discussion\n",
        "Write 1\u20132 short paragraphs summarizing your results: performance, any signs of concentration, and whether feature\u2011map changes improved or degraded the kernel and accuracy.\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feedback to us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Optional question\n",
        "Was there any part of the tasks where you struggled for some \"unnecessary\" reason? (Errors in the notebook, bad instructions etc.)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your optional answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Disclosure of AI Usage (Mandatory)\n",
        "Fill in this part disclosing any AI usage before submitting the assignment by describing your use of LLMs or other AI-based tools in this assignment.\n",
        "\n",
        "For each task, we ask you to provide information about:\n",
        "- **Tools/models used**.\n",
        "- **Per\u2011task usage**: for each task, a brief summary of what the tool was used for.\n",
        "- **Prompts/transcripts**: main prompts or a summary of interactions (a link or screenshot is acceptable if long).\n",
        "- **Validation**: how you checked and verified the correctness of AI-generated outputs (tests run, docs consulted, comparisons, plots etc.).\n",
        "\n",
        "Disclosure:\n",
        "- **Task 0**: [...describe your use...]\n",
        "\n",
        "- **Task 1**: [...describe your use...]\n",
        "\n",
        "- **Task 2**: [...describe your use...]\n",
        "\n",
        "- **Task 3**: [...describe your use...]\n",
        "\n",
        "- **Task 4**: [...describe your use...]\n",
        "\n",
        "- **Task 5**: [...describe your use...]\n",
        "\n",
        "- **Task 6**: [...describe your use...]\n",
        "\n",
        "- **Bonus A**: [...describe your use...]\n",
        "\n",
        "- **Bonus B**: [...describe your use...]\n",
        "\n",
        "If you did not use any AI tools for a given exercise, specify this by writing \"None\". If you did not complete the bonus exercises, you can leave those fields empty."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}